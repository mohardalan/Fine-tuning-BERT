{"cells":[{"cell_type":"code","execution_count":null,"id":"10702b1c","metadata":{"id":"10702b1c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"otWLXzm4MtR_","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14303,"status":"ok","timestamp":1706192373430,"user":{"displayName":"Mohammad Ardalan","userId":"15069546489890655655"},"user_tz":300},"id":"otWLXzm4MtR_","outputId":"4a3c02b6-54c8-4add-b7c6-b24d00663dd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"5b508299","metadata":{"id":"5b508299"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score"]},{"cell_type":"code","execution_count":null,"id":"hO4LDV_vNYdI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1706195340273,"user":{"displayName":"Mohammad Ardalan","userId":"15069546489890655655"},"user_tz":300},"id":"hO4LDV_vNYdI","outputId":"0e4da252-e971-4123-9b9d-481ed04393fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current working directory: /content/drive/MyDrive/Colab Notebooks\n"]}],"source":["import os\n","\n","# Define the new directory path\n","new_directory = '/content/drive/MyDrive/Colab Notebooks'\n","\n","# Change the current working directory to the new one\n","os.chdir(new_directory)\n","\n","# Verify the change by printing the current working directory\n","print(\"Current working directory:\", os.getcwd())"]},{"cell_type":"code","execution_count":null,"id":"fe03d7f9","metadata":{"id":"fe03d7f9"},"outputs":[],"source":["# Load the CSV file into a DataFrame\n","df = pd.read_csv('Datasets/data_amazon_product_reviews_video_games.csv')"]},{"cell_type":"code","execution_count":null,"id":"2978cb28","metadata":{"id":"2978cb28"},"outputs":[],"source":["df.drop(labels= ['Unnamed: 0', 'reviewerID', 'asin', 'reviewerName', 'helpful',\n","       'unixReviewTime', 'reviewTime'], axis= 1, inplace= True)"]},{"cell_type":"code","execution_count":null,"id":"5c4624c6","metadata":{"id":"5c4624c6"},"outputs":[],"source":["df.dropna(inplace= True)\n","#df.isna().sum()"]},{"cell_type":"code","execution_count":null,"id":"7eea3396","metadata":{"id":"7eea3396"},"outputs":[],"source":["df['overall']= df['overall'].astype(dtype= 'int64')\n","#df.info()"]},{"cell_type":"code","execution_count":null,"id":"6pN1wlPqk4GB","metadata":{"id":"6pN1wlPqk4GB"},"outputs":[],"source":["df['new_text']= df['reviewText'] + ' ' + df['summary']"]},{"cell_type":"code","execution_count":null,"id":"I1IJ1E16ktKB","metadata":{"id":"I1IJ1E16ktKB"},"outputs":[],"source":["texts= df['new_text'].tolist()\n","labels= df['overall'].tolist()"]},{"cell_type":"code","execution_count":null,"id":"2258c95f","metadata":{"id":"2258c95f"},"outputs":[],"source":["# Define hyperparameters\n","batch_size = 16"]},{"cell_type":"code","execution_count":null,"id":"d86fc126","metadata":{"id":"d86fc126"},"outputs":[],"source":["# Split the data into train and test sets (80% train, 20% test)\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Further split the test set into dev and test sets (50% dev, 50% test)\n","dev_texts, test_texts, dev_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"id":"58ee2c76","metadata":{"id":"58ee2c76"},"outputs":[],"source":["# Load the pre-trained BERT tokenizer and model\n","checkpoint= \"LiYuan/amazon-review-sentiment-analysis\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"fdad104a","metadata":{"id":"fdad104a"},"outputs":[],"source":["# Tokenize the input texts\n","#tokenized_train_texts = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n","#tokenized_dev_texts = tokenizer(dev_texts, padding=True, truncation=True, return_tensors='pt')\n","tokenized_test_texts = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"id":"12d8bc6d","metadata":{"id":"12d8bc6d"},"outputs":[],"source":["# Convert the labels to tensor\n","#train_labels = torch.tensor(train_labels)\n","#dev_labels = torch.tensor(dev_labels)\n","test_labels = torch.tensor(test_labels)"]},{"cell_type":"code","execution_count":null,"id":"f60df1a3","metadata":{"id":"f60df1a3","scrolled":true},"outputs":[],"source":["# Create TensorDatasets and DataLoaders for train, dev, and test sets\n","#train_dataset = TensorDataset(tokenized_train_texts['input_ids'], tokenized_train_texts['attention_mask'], train_labels)\n","#dev_dataset = TensorDataset(tokenized_dev_texts['input_ids'], tokenized_dev_texts['attention_mask'], dev_labels)\n","test_dataset = TensorDataset(tokenized_test_texts['input_ids'], tokenized_test_texts['attention_mask'], test_labels)"]},{"cell_type":"code","execution_count":null,"id":"c9f26ea4","metadata":{"id":"c9f26ea4"},"outputs":[],"source":["#train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n","#dev_dataloader = DataLoader(dev_dataset, batch_size= batch_size, shuffle= False)\n","test_dataloader = DataLoader(test_dataset, batch_size= batch_size, shuffle= False, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"id":"5b260ad1","metadata":{"id":"5b260ad1"},"outputs":[],"source":["# Load the pre-trained BERT model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"id":"bd495a86","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151132,"status":"ok","timestamp":1706197645328,"user":{"displayName":"Mohammad Ardalan","userId":"15069546489890655655"},"user_tz":300},"id":"bd495a86","outputId":"e92f01be-ae94-483e-a573-f78bd692a694"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testset accuracy: 0.68494 , Testset F1 score: 0.67219, Average loss: 0.79407\n"]}],"source":["# Evaluate on the test set\n","model.eval()\n","y_true_test = []\n","y_pred_test = []\n","loss_epoch= []\n","\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, batch_labels = batch\n","\n","        input_ids= input_ids.to(device)\n","        attention_mask= attention_mask.to(device)\n","        batch_labels= batch_labels.to(device)\n","\n","        outputs = model(input_ids= input_ids, attention_mask= attention_mask)\n","        logits = outputs.logits\n","        _, predicted = torch.max(logits, 1)\n","\n","        # Append true labels and predicted labels for later use\n","        y_true_test.extend(batch_labels.tolist())\n","        y_pred_test.extend(predicted.tolist())\n","\n","        # Calculate the loss\n","        loss = criterion(logits, batch_labels)\n","        loss_epoch.append(loss)\n","\n","# Calculate accuracy and F1 score for the test set\n","test_accuracy = accuracy_score(y_true_test, y_pred_test)\n","test_f1 = f1_score(y_true_test, y_pred_test, average='weighted')\n","\n","# Calculate the average loss\n","loss_epoch_np = [tensor.cpu().detach().numpy() for tensor in loss_epoch]\n","average_loss= np.mean(loss_epoch_np)\n","#average_loss= np.mean(loss_epoch)\n","\n","print(f\"Testset accuracy: {round(test_accuracy,5)} , Testset F1 score: {round(test_f1,5)}, Average loss: {round(average_loss.tolist(),5)}\")\n","Test_results= [test_accuracy, test_f1, average_loss]"]},{"cell_type":"code","execution_count":null,"id":"31d2ccd6","metadata":{"id":"31d2ccd6"},"outputs":[],"source":["# Saving Testset Results\n","data = {\n","    'Test_results': Test_results\n","}\n","df = pd.DataFrame(data)\n","df.to_csv('Outputs/LiYuan Model-Source_Test_results.csv', index= False)"]},{"cell_type":"code","execution_count":null,"id":"817c267c","metadata":{"id":"817c267c"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
