{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10702b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b508299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AdamW, get_scheduler, AutoModel\n",
    "\n",
    "from pyswarms.single.global_best import GlobalBestPSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe03d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('Datasets/data_amazon_product_reviews_video_games.csv')\n",
    "\n",
    "df.drop(labels= ['Unnamed: 0', 'reviewerID', 'asin', 'reviewerName', 'helpful',\n",
    "       'unixReviewTime', 'reviewTime'], axis= 1, inplace= True)\n",
    "\n",
    "df.dropna(inplace= True)\n",
    "\n",
    "df['overall']= df['overall'].astype(dtype= 'int64')\n",
    "\n",
    "df['new_text']= df['reviewText'] + ' ' + df['summary']\n",
    "\n",
    "# Limitation of size of data\n",
    "df = df.sample(n=1000, random_state=42)\n",
    "\n",
    "texts= df['new_text'].tolist()\n",
    "labels= df['overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86fc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size= 0.2, random_state= 42)\n",
    "\n",
    "# Further split the test set into dev and test sets (50% dev, 50% test)\n",
    "dev_texts, test_texts, dev_labels, test_labels = train_test_split(test_texts, test_labels, test_size= 0.5, random_state=42)\n",
    "\n",
    "# Load the pre-trained BERT tokenizer and model\n",
    "checkpoint= \"LiYuan/amazon-review-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenized_train_texts = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_dev_texts = tokenizer(dev_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_test_texts = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Convert the labels to tensor\n",
    "train_labels = torch.tensor(train_labels)\n",
    "dev_labels = torch.tensor(dev_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for train, dev, and test sets\n",
    "train_dataset = TensorDataset(tokenized_train_texts['input_ids'], tokenized_train_texts['attention_mask'], train_labels)\n",
    "dev_dataset = TensorDataset(tokenized_dev_texts['input_ids'], tokenized_dev_texts['attention_mask'], dev_labels)\n",
    "test_dataset = TensorDataset(tokenized_test_texts['input_ids'], tokenized_test_texts['attention_mask'], test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f26ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model architecture\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(bert_model.config.hidden_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids= input_ids, attention_mask= attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6e1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(params):\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    dropout_rate = 0.1   # Dropout ratio of the additional hidden layer\n",
    "    hidden_dim = 256   # 256 classes as an additional hidden layer\n",
    "    output_dim = 5    # 5 classes for sentiment analysis\n",
    "\n",
    "    Costs = np.zeros(params.shape[0])  # x.shape[0] gives the number of particles\n",
    "    for i in range(params.shape[0]):\n",
    "        learning_rate= params[i, 0]\n",
    "        batch_size= int(params[i, 1])\n",
    "        weight_decay= params[i, 2]\n",
    "        num_epochs= int(params[i, 3])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size= batch_size, shuffle= False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size= batch_size, shuffle= False)\n",
    "        \n",
    "    # LiYuan Model\n",
    "    bert_model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "    # Freeze the BERT model parameters\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Create an instance of the SentimentClassifier\n",
    "    model = SentimentClassifier(bert_model, hidden_dim, output_dim, dropout_rate)\n",
    "        \n",
    "    # Define the optimizer for training the softmax layer\n",
    "    optimizer = optim.Adam(model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "        \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, batch_labels = batch\n",
    "                \n",
    "            input_ids= input_ids.to(device)\n",
    "            attention_mask= attention_mask.to(device)\n",
    "            batch_labels= batch_labels.to(device)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids= input_ids, attention_mask= attention_mask)\n",
    "            loss = criterion(outputs, batch_labels)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        dev_correct = 0\n",
    "        total_dev = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        loss_epoch= []\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_dataloader:\n",
    "                input_ids, attention_mask, batch_labels = batch\n",
    "                    \n",
    "                input_ids= input_ids.to(device)\n",
    "                attention_mask= attention_mask.to(device)\n",
    "                batch_labels= batch_labels.to(device)\n",
    "                    \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "\n",
    "                # Append true labels and predicted labels for later use\n",
    "                y_true.extend(batch_labels.tolist())\n",
    "                y_pred.extend(predicted.tolist())\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(logits, batch_labels)\n",
    "                loss_epoch.append(loss)\n",
    "                    \n",
    "            \n",
    "        # Calculate accuracy and F1 score\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        loss_epoch_np = [tensor.cpu().detach().numpy() for tensor in loss_epoch]\n",
    "        average_loss= np.mean(loss_epoch_np)\n",
    "            \n",
    "        print(f'learning_rate: {learning_rate}, batch_size: {batch_size}, weight_decay: {weight_decay}, num_epochs: {num_epochs}')\n",
    "        print(f'epoch No. : {epoch}, Devset Accuracy : {round(accuracy,5)}, Devset f1_score : {round(f1,5)}, Average loss: {round(average_loss.tolist(),5)}')\n",
    "        print()\n",
    "    validation_loss= average_loss\n",
    "\n",
    "    Costs[i]= (1/f1)**2\n",
    "    \n",
    "    # Return the validation loss as the objective value\n",
    "    return Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce643b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters #learning_rate = 1e-3 #batch_size = 16 #weight_decay = 1e-4 #num_epochs = 1\n",
    "\n",
    "# Define the bounds for the hyperparameters\n",
    "lower_bound = np.array([1e-4, 16, 1e-5, 1])\n",
    "upper_bound = np.array([1e-2, 256, 1e-3, 2])\n",
    "bounds = (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b260ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 12:02:28,387 - pyswarms.single.global_best - INFO - Optimize for 1 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   0%|          |0/1Some weights of the model checkpoint at LiYuan/amazon-review-sentiment-analysis were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.0016880306737898222, batch_size: 222, weight_decay: 0.0009419545918603944, num_epochs: 1\n",
      "epoch No. : 0, Devset Accuracy : 0.8, Devset f1_score : 0.79667, Average loss: 1.32128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at LiYuan/amazon-review-sentiment-analysis were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.009231734454570178, batch_size: 190, weight_decay: 9.018604274760619e-05, num_epochs: 1\n",
      "epoch No. : 0, Devset Accuracy : 0.1, Devset f1_score : 0.05, Average loss: 1.67766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at LiYuan/amazon-review-sentiment-analysis were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "pyswarms.single.global_best: 100%|██████████|1/1, best_cost=2.58\n",
      "2024-01-25 12:08:17,363 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 2.5765079823497947, best pos: [1.68803067e-03 2.22008745e+02 9.41954592e-04 1.20808781e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.008593180470999665, batch_size: 28, weight_decay: 0.00046871301764920376, num_epochs: 1\n",
      "epoch No. : 0, Devset Accuracy : 0.1, Devset f1_score : 0.06667, Average loss: 1.4169\n",
      "\n",
      "Best position: [1.68803067e-03 2.22008745e+02 9.41954592e-04 1.20808781e+00]\n",
      "Best cost: 2.5765079823497947\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "options={'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "optimizer = GlobalBestPSO(n_particles= 3, dimensions= 4, options=options, bounds=bounds)\n",
    "\n",
    "# Run the optimization\n",
    "best_costs, best_hyperparams = optimizer.optimize(objective_function, iters= 1)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best position:\", best_hyperparams)\n",
    "print(\"Best cost:\", best_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9f53f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0016880306737898222, 222, 0.0009419545918603944, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate= best_hyperparams[0]\n",
    "batch_size= int(best_hyperparams[1])\n",
    "weight_decay= best_hyperparams[2]\n",
    "num_epochs= int(best_hyperparams[3])\n",
    "\n",
    "learning_rate, batch_size, weight_decay, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e93704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "dropout_rate = 0.1   # Dropout ratio of the additional hidden layer\n",
    "hidden_dim = 256   # 256 classes as an additional hidden layer\n",
    "output_dim = 5    # 5 classes for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4fe01ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at LiYuan/amazon-review-sentiment-analysis were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch No. : 0, Devset Accuracy : 0.8, Devset f1_score : 0.71111, Average loss: 1.38346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size= batch_size, shuffle= False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= batch_size, shuffle= False)\n",
    "\n",
    "# LiYuan Model\n",
    "bert_model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Freeze the BERT model parameters\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create an instance of the SentimentClassifier\n",
    "model = SentimentClassifier(bert_model, hidden_dim, output_dim, dropout_rate)\n",
    "\n",
    "# Define the optimizer for training the softmax layer\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "best_dev_accuracy = 0.0\n",
    "best_model_state_dict = None\n",
    "Validation_results= []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, batch_labels = batch\n",
    "        \n",
    "        input_ids= input_ids.to(device)\n",
    "        attention_mask= attention_mask.to(device)\n",
    "        batch_labels= batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids= input_ids, attention_mask= attention_mask)\n",
    "        loss = criterion(outputs, batch_labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    dev_correct = 0\n",
    "    total_dev = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_epoch= []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            input_ids, attention_mask, batch_labels = batch\n",
    "            \n",
    "            input_ids= input_ids.to(device)\n",
    "            attention_mask= attention_mask.to(device)\n",
    "            batch_labels= batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            # Append true labels and predicted labels for later use\n",
    "            y_true.extend(batch_labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            loss_epoch.append(loss)        \n",
    "        \n",
    "    # Calculate accuracy and F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate the average loss\n",
    "    loss_epoch_np = [tensor.cpu().detach().numpy() for tensor in loss_epoch]\n",
    "    average_loss= np.mean(loss_epoch_np)\n",
    "    \n",
    "    print(f'epoch No. : {epoch}, Devset Accuracy : {round(accuracy,5)}, Devset f1_score : {round(f1,5)}, Average loss: {round(average_loss.tolist(),5)}')\n",
    "    \n",
    "    Validation_results.append([accuracy, f1, average_loss])\n",
    "    \n",
    "    if accuracy > best_dev_accuracy:\n",
    "        best_dev_accuracy = accuracy\n",
    "        # Save the best model (optional)\n",
    "        best_model_state_dict = model.state_dict()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a00bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Testset Results\n",
    "data = {\n",
    "    'Validation_results': Validation_results,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Outputs/Main-9-LiYuan-Two layers-OPT_Validation_results.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8712d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model state dict\n",
    "if best_model_state_dict is not None:\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "    # Define the directory path to save the model\n",
    "    save_path = 'Saved Models/Main-9-LiYuan-Two layers-OPT.pth'  \n",
    "\n",
    "    # Save the model state dictionary and other relevant information\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state_dict,\n",
    "        'tokenizer': tokenizer  \n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb182413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset accuracy: 0.6 , Testset F1 score: 0.45, Average loss: 1.43218\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "y_true_test = []\n",
    "y_pred_test = []\n",
    "loss_epoch= []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, batch_labels = batch\n",
    "        \n",
    "        input_ids= input_ids.to(device)\n",
    "        attention_mask= attention_mask.to(device)\n",
    "        batch_labels= batch_labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids= input_ids, attention_mask= attention_mask)\n",
    "        logits = outputs\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "\n",
    "        # Append true labels and predicted labels for later use\n",
    "        y_true_test.extend(batch_labels.tolist())\n",
    "        y_pred_test.extend(predicted.tolist())\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss_epoch.append(loss)\n",
    "\n",
    "# Calculate accuracy and F1 score for the test set\n",
    "test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "test_f1 = f1_score(y_true_test, y_pred_test, average='weighted')\n",
    "\n",
    "# Calculate the average loss\n",
    "loss_epoch_np = [tensor.cpu().detach().numpy() for tensor in loss_epoch]\n",
    "average_loss= np.mean(loss_epoch_np)\n",
    "\n",
    "print(f\"Testset accuracy: {round(test_accuracy,5)} , Testset F1 score: {round(test_f1,5)}, Average loss: {round(average_loss.tolist(),5)}\")\n",
    "Test_results= [test_accuracy, test_f1, average_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a018216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Testset Results\n",
    "data = {\n",
    "    'Test_results': Test_results\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Outputs/Main-9-LiYuan-Two layers-OPT_Test_results.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873084e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
